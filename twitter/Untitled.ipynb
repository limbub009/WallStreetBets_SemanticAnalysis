{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\": [{\"id\": \"1520256008483921920\", \"value\": \"appl\", \"tag\": \"appl\"}], \"meta\": {\"sent\": \"2022-04-30T04:33:19.696Z\", \"result_count\": 1}}\n",
      "{\"meta\": {\"sent\": \"2022-04-30T04:33:21.049Z\", \"summary\": {\"deleted\": 1, \"not_deleted\": 0}}}\n",
      "{\"data\": [{\"value\": \"appl\", \"tag\": \"appl\", \"id\": \"1520259989692198912\"}], \"meta\": {\"sent\": \"2022-04-30T04:33:22.687Z\", \"summary\": {\"created\": 1, \"not_created\": 0, \"valid\": 1, \"invalid\": 0}}}\n",
      "200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 105>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    103\u001b[0m     get_stream(headers, \u001b[38;5;28mset\u001b[39m, bearer_token)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 106\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    101\u001b[0m delete \u001b[38;5;241m=\u001b[39m delete_all_rules(headers, bearer_token, rules)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mset\u001b[39m \u001b[38;5;241m=\u001b[39m set_rules(headers, delete, bearer_token)\n\u001b[1;32m--> 103\u001b[0m \u001b[43mget_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbearer_token\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mget_stream\u001b[1;34m(headers, set, bearer_token)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot get stream (HTTP \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     80\u001b[0m             response\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m     81\u001b[0m         )\n\u001b[0;32m     82\u001b[0m     )\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response_line \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines():\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response_line:\n\u001b[0;32m     85\u001b[0m         json_response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response_line)\n",
      "File \u001b[1;32mc:\\users\\bishes limbu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\requests\\models.py:804\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[1;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;124;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;124;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[0;32m    797\u001b[0m \u001b[38;5;124;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[0;32m    798\u001b[0m \n\u001b[0;32m    799\u001b[0m \u001b[38;5;124;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    802\u001b[0m pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 804\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size, decode_unicode\u001b[38;5;241m=\u001b[39mdecode_unicode):\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pending \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    807\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m pending \u001b[38;5;241m+\u001b[39m chunk\n",
      "File \u001b[1;32mc:\\users\\bishes limbu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\requests\\models.py:760\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    759\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 760\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    761\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\users\\bishes limbu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\urllib3\\response.py:575\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m--> 575\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content):\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m line\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\users\\bishes limbu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\urllib3\\response.py:767\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    766\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 767\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    769\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\bishes limbu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\urllib3\\response.py:697\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 697\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    698\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\users\\bishes limbu\\appdata\\local\\programs\\python\\python38\\lib\\socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\bishes limbu\\appdata\\local\\programs\\python\\python38\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\users\\bishes limbu\\appdata\\local\\programs\\python\\python38\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import config\n",
    "import preprocessor as p #cleaner\n",
    "from langdetect import detect #language detecter\n",
    "from csv import writer #write to a csv file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# To set your enviornment variables in your terminal run the following line:\n",
    "# export 'BEARER_TOKEN'='<your_bearer_token>'\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def get_rules(headers, bearer_token):\n",
    "    response = requests.get(\n",
    "        \"https://api.twitter.com/2/tweets/search/stream/rules\", headers=headers\n",
    "    )\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Cannot get rules (HTTP {}): {}\".format(response.status_code, response.text)\n",
    "        )\n",
    "    print(json.dumps(response.json()))\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def delete_all_rules(headers, bearer_token, rules):\n",
    "    if rules is None or \"data\" not in rules:\n",
    "        return None\n",
    "\n",
    "    ids = list(map(lambda rule: rule[\"id\"], rules[\"data\"]))\n",
    "    payload = {\"delete\": {\"ids\": ids}}\n",
    "    response = requests.post(\n",
    "        \"https://api.twitter.com/2/tweets/search/stream/rules\",\n",
    "        headers = headers,\n",
    "        json=payload\n",
    "    )\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Cannot delete rules (HTTP {}): {}\".format(\n",
    "                response.status_code, response.text\n",
    "            )\n",
    "        )\n",
    "    print(json.dumps(response.json()))\n",
    "\n",
    "\n",
    "def set_rules(headers,delete,bearer_token):\n",
    "    # You can adjust the rules if needed\n",
    "    sample_rules = [\n",
    "        {\"value\": \"appl\", \"tag\": \"appl\"},\n",
    "       # {\"value\": \"cat has:images -grumpy\", \"tag\": \"cat pictures\"},\n",
    "    ]\n",
    "    payload = {\"add\": sample_rules}\n",
    "    response = requests.post(\n",
    "        \"https://api.twitter.com/2/tweets/search/stream/rules\",\n",
    "        headers=headers,\n",
    "        json=payload,\n",
    "    )\n",
    "    if response.status_code != 201:\n",
    "        raise Exception(\n",
    "            \"Cannot add rules (HTTP {}): {}\".format(response.status_code, response.text)\n",
    "        )\n",
    "    print(json.dumps(response.json()))\n",
    "\n",
    "\n",
    "def get_stream(headers,set,bearer_token):\n",
    "    response = requests.get(\n",
    "        \"https://api.twitter.com/2/tweets/search/stream\", headers=headers, stream=True,\n",
    "    )\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Cannot get stream (HTTP {}): {}\".format(\n",
    "                response.status_code, response.text\n",
    "            )\n",
    "        )\n",
    "    for response_line in response.iter_lines():\n",
    "        if response_line:\n",
    "            json_response = json.loads(response_line)\n",
    "            #print(json.dumps(json_response, indent=4, sort_keys=True))\n",
    "            tweet = json_response['data']['text']\n",
    "            tweet = p.clean(tweet) #cleaner\n",
    "            #print(tweet)\n",
    "            try: #compatible tweets with detect\n",
    "                if detect(tweet) == 'en':\n",
    "                    print(tweet)\n",
    "                    tweetlst = [tweet]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "def main():\n",
    "    bearer_token = config.BEARER_TOKEN\n",
    "    headers = create_headers(bearer_token)\n",
    "    rules = get_rules(headers, bearer_token)\n",
    "    delete = delete_all_rules(headers, bearer_token, rules)\n",
    "    set = set_rules(headers, delete, bearer_token)\n",
    "    get_stream(headers, set, bearer_token)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Sentiment Text Sentiment\n",
      "0    : has now transfered $4,000,000,000,000 around...   Bullish\n",
      "1    Bitcoin next move....? In the next comming month.   Bullish\n",
      "2    If you have a phone or a computer, why not let...   Neutral\n",
      "3    Pump Alert! - UTK/USDT on Binance! Price incre...   Neutral\n",
      "4    Crypto Pump Alert! - Frankie WSB (cancelled) s...   Neutral\n",
      "..                                                 ...       ...\n",
      "244                        get out before they doom us   Bearish\n",
      "245                        help me its too late for me   Bearish\n",
      "246                         APPL giving me a headache    Bearish\n",
      "247                     the earnings are out tomorrow    Neutral\n",
      "248               pray for me, help me out of this pit   Bearish\n",
      "\n",
      "[249 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from ernie import SentenceClassifier, Models\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = {'Bearish': 0, 'Neutral': 1, 'Bullish': 2}\n",
    "df.Sentiment = df.Sentiment.map(mapper)\n",
    "\n",
    "#maps the sentiment to numbers \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Sentiment Text  Sentiment\n",
      "0    : has now transfered $4,000,000,000,000 around...          2\n",
      "1    Bitcoin next move....? In the next comming month.          2\n",
      "2    If you have a phone or a computer, why not let...          1\n",
      "3    Pump Alert! - UTK/USDT on Binance! Price incre...          1\n",
      "4    Crypto Pump Alert! - Frankie WSB (cancelled) s...          1\n",
      "..                                                 ...        ...\n",
      "244                        get out before they doom us          0\n",
      "245                        help me its too late for me          0\n",
      "246                         APPL giving me a headache           0\n",
      "247                     the earnings are out tomorrow           1\n",
      "248               pray for me, help me out of this pit          0\n",
      "\n",
      "[249 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print (df)\n",
    "df.Sentiment.value_counts()\n",
    "df.columns = ['SentimentText','Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean and shuffle data\n",
    "df.SentimentText = df.SentimentText.str.replace(':',\"\")\n",
    "#replace colons from tweets added from stream\n",
    "df = df.sample(frac=1).reset_index(drop=True)#randomise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         SentimentText  Sentiment\n",
      "0    is booming Will it last, or will it bust? via ...          0\n",
      "1     THE CRYPTO GIVEAWAY CONTEST IS NOW LIVE! $25K...          1\n",
      "2     $ARDR is a multichain platform with a parent-...          1\n",
      "3         An alternate way to look at price over time.          1\n",
      "4    Earn a share of million $REAP cryptocurrency i...          1\n",
      "..                                                 ...        ...\n",
      "244  Sorry for bothering you. But I actually have a...          1\n",
      "245               closed above $50k for days in a row!          2\n",
      "246                    Still got my eyes on the prize.          2\n",
      "247   New Listing will list - Trading pair DRYCAKE/...          1\n",
      "248  Bitcoin next move....? In the next comming month.          2\n",
      "\n",
      "[249 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import classifier\n",
    "classifier = SentenceClassifier(model_name='bert-base-uncased', max_length=64, labels_no=3)\n",
    "classifier.load_dataset(df, validation_split=0.1)\n",
    "classifier.fine_tune(epochs=10, learning_rate=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset into a format classifier can understand\n",
    "classifier.load_dataset(df, validation_split=0.1)\n",
    "#train on 90% of the data and test on 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1734 - accuracy: 0.9688WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 25 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 03:49:24 [WARNING] Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 25 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 38s 4s/step - loss: 0.1734 - accuracy: 0.9688 - val_loss: 0.7348 - val_accuracy: 0.7600\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1463 - accuracy: 0.9598WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 25 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 03:49:50 [WARNING] Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 25 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 25s 4s/step - loss: 0.1463 - accuracy: 0.9598 - val_loss: 0.7076 - val_accuracy: 0.8000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1297 - accuracy: 0.9643WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 25 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 03:50:15 [WARNING] Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 25 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 25s 4s/step - loss: 0.1297 - accuracy: 0.9643 - val_loss: 0.8379 - val_accuracy: 0.7200\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9732WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 25 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 03:50:42 [WARNING] Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 25 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 27s 4s/step - loss: 0.1014 - accuracy: 0.9732 - val_loss: 0.8487 - val_accuracy: 0.7600\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.9732WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 25 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 03:51:11 [WARNING] Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 25 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 29s 4s/step - loss: 0.0911 - accuracy: 0.9732 - val_loss: 0.8340 - val_accuracy: 0.7200\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0840 - accuracy: 0.9777WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 25 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 03:51:39 [WARNING] Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 25 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 28s 4s/step - loss: 0.0840 - accuracy: 0.9777 - val_loss: 0.8057 - val_accuracy: 0.7200\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 0.9688WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 25 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 03:52:06 [WARNING] Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 25 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 27s 4s/step - loss: 0.0819 - accuracy: 0.9688 - val_loss: 0.7788 - val_accuracy: 0.7600\n",
      "3/7 [===========>..................] - ETA: 14s - loss: 0.0509 - accuracy: 0.9896"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#fine tune classifier\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfine_tune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\bishes limbu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ernie\\ernie.py:143\u001b[0m, in \u001b[0;36mSentenceClassifier.fine_tune\u001b[1;34m(self, epochs, learning_rate, epsilon, clipnorm, optimizer_function, optimizer_kwargs, loss_function, loss_kwargs, accuracy_function, accuracy_kwargs, training_batch_size, validation_batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m temporary_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_temporary_path(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# The fine-tuned model does not have the same input interface after being\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# exported and loaded again.\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reload_model()\n",
      "File \u001b[1;32mc:\\users\\bishes limbu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1178\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1171\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1173\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1174\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1175\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1176\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1177\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1178\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1179\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1180\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\users\\bishes limbu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:889\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 889\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    891\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    892\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\users\\bishes limbu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    915\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    920\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    921\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\users\\bishes limbu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3023\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3020\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   3021\u001b[0m   (graph_function,\n\u001b[0;32m   3022\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\bishes limbu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1960\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1956\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1958\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1959\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1960\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1961\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1962\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1963\u001b[0m     args,\n\u001b[0;32m   1964\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1965\u001b[0m     executing_eagerly)\n\u001b[0;32m   1966\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\users\\bishes limbu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    599\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    604\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\users\\bishes limbu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#fine tune classifier\n",
    "print(\"training\")\n",
    "classifier.fine_tune(epochs=10, learning_rate=2e-5) #learning rate from documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         SentimentText  Sentiment\n",
      "0    is booming Will it last, or will it bust? via ...          0\n",
      "1     THE CRYPTO GIVEAWAY CONTEST IS NOW LIVE! $25K...          1\n",
      "2     $ARDR is a multichain platform with a parent-...          1\n",
      "3         An alternate way to look at price over time.          1\n",
      "4    Earn a share of million $REAP cryptocurrency i...          1\n",
      "..                                                 ...        ...\n",
      "244  Sorry for bothering you. But I actually have a...          1\n",
      "245               closed above $50k for days in a row!          2\n",
      "246                    Still got my eyes on the prize.          2\n",
      "247   New Listing will list - Trading pair DRYCAKE/...          1\n",
      "248  Bitcoin next move....? In the next comming month.          2\n",
      "\n",
      "[249 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#loss to go down and accurray to go up\n",
    "\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSFT earnings are out tomorrow\n",
      "========================================\n",
      "(0.011232829939373159, 0.9774283373854925, 0.011338832675134369)\n",
      "========================================\n",
      "Neutral\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "text = \"MSFT earnings are out tomorrow\"\n",
    "\n",
    "print(text)\n",
    "print(\"========================================\")\n",
    "probabilities = classifier.predict_one(text)\n",
    "print(probabilities)\n",
    "classes = [\"Bearish\", \"Neutral\", \"Bullish\"]\n",
    "print(\"========================================\")\n",
    "print(classes[np.argmax(probabilities)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The big day has come! This Sunday we are launching our own amazing NFT platform! TO THE MOOOON!!\n",
      "========================================\n",
      "(0.006389703190386593, 0.26059921186461965, 0.7330110849449938)\n",
      "========================================\n",
      "Bullish\n"
     ]
    }
   ],
   "source": [
    "text = \"The big day has come! This Sunday we are launching our own amazing NFT platform! TO THE MOOOON!!\"\n",
    "\n",
    "print(text)\n",
    "print(\"========================================\")\n",
    "probabilities = classifier.predict_one(text)\n",
    "print(probabilities)\n",
    "classes = [\"Bearish\", \"Neutral\", \"Bullish\"]\n",
    "print(\"========================================\")\n",
    "print(classes[np.argmax(probabilities)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neither a rich man or a poor man gets to enjoy his money. One is too busy trying to hold onto it, and the other is too busy trying to get it . Live laugh & love as much as you can ðŸ’—\n",
      "========================================\n",
      "(0.00313096557735057, 0.8702835638637582, 0.1265854705588911)\n",
      "========================================\n",
      "Neutral\n"
     ]
    }
   ],
   "source": [
    "text = \"Neither a rich man or a poor man gets to enjoy his money. One is too busy trying to hold onto it, and the other is too busy trying to get it . Live laugh & love as much as you can ðŸ’—\"\n",
    "\n",
    "print(text)\n",
    "print(\"========================================\")\n",
    "probabilities = classifier.predict_one(text)\n",
    "print(probabilities)\n",
    "classes = [\"Bearish\", \"Neutral\", \"Bullish\"]\n",
    "print(\"========================================\")\n",
    "print(classes[np.argmax(probabilities)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markets may rally first to make the stage for a big gains.\n",
      "========================================\n",
      "(0.012845361417848634, 0.015881746245201476, 0.9712728923369499)\n",
      "========================================\n",
      "Bullish\n"
     ]
    }
   ],
   "source": [
    "text = \"Markets may rally first to make the stage for a big gains.\"\n",
    "\n",
    "print(text)\n",
    "print(\"========================================\")\n",
    "probabilities = classifier.predict_one(text)\n",
    "print(probabilities)\n",
    "classes = [\"Bearish\", \"Neutral\", \"Bullish\"]\n",
    "print(\"========================================\")\n",
    "print(classes[np.argmax(probabilities)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "#trigger launch script to stream tweets from django\n",
    "#timer using try and except\n",
    "#append the tweets/semantic analysis to a array\n",
    "#display on html page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_headers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m<\u001b[39m t_end:\n\u001b[0;32m      5\u001b[0m     bearer_token \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mBEARER_TOKEN\n\u001b[1;32m----> 6\u001b[0m     headers \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_headers\u001b[49m(bearer_token)\n\u001b[0;32m      7\u001b[0m     rules \u001b[38;5;241m=\u001b[39m get_rules(headers, bearer_token)\n\u001b[0;32m      8\u001b[0m     delete \u001b[38;5;241m=\u001b[39m delete_all_rules(headers, bearer_token, rules)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_headers' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t_end = time.time() + 10\n",
    "while time.time() < t_end:\n",
    "    bearer_token = config.BEARER_TOKEN\n",
    "    headers = create_headers(bearer_token)\n",
    "    rules = get_rules(headers, bearer_token)\n",
    "    delete = delete_all_rules(headers, bearer_token, rules)\n",
    "    set = set_rules(headers, delete, bearer_token)\n",
    "    get_stream(headers, set, bearer_token)\n",
    "print(\"ended\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_endpoint(url, headers):\n",
    "    response = requests.request(\"GET\", url, headers=headers, stream=True)\n",
    "    print(response.status_code)\n",
    "    for response_line in response.iter_lines():\n",
    "        if response_line:\n",
    "            json_response = json.loads(response_line)\n",
    "            print(json.dumps(json_response, indent=4, sort_keys=True))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Request returned an error: {} {}\".format(\n",
    "                response.status_code, response.text\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    print(\"here\")\n",
    "    url = create_url()\n",
    "    bearer_token = config.BEARER_TOKEN\n",
    "    headers = create_headers(bearer_token)\n",
    "    rules = get_rules(headers, bearer_token)\n",
    "    delete = delete_all_rules(headers, bearer_token, rules)\n",
    "    set = set_rules(headers, delete, bearer_token)\n",
    "    timeout = 0\n",
    "    while True:\n",
    "        connect_to_endpoint(url, headers)\n",
    "        print(\"connected\")\n",
    "        timeout += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
